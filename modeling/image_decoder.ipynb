{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-07T01:12:02.669903800Z",
     "start_time": "2023-06-07T01:12:01.763950500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import List, Tuple, Type\n",
    "\n",
    "from common import LayerNorm2d\n",
    "from help_func import print_var_detail\n",
    "\n",
    "\n",
    "class ImageDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        transformer_dim: int,\n",
    "        transformer: nn.Module,\n",
    "        num_params: int=4,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "        output_dim_factor: int = 8,\n",
    "        SSIM_head_depth: int = 3,\n",
    "        SSIM_head_hidden_dim: int = 256,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Predicts masks given an image and prompt embeddings, using a\n",
    "        transformer architecture.\n",
    "\n",
    "        Arguments:\n",
    "          transformer_dim (int): the channel dimension of the transformer\n",
    "          transformer (nn.Module): the transformer used to predict masks\n",
    "          num_multimask_outputs (int): the number of masks to predict\n",
    "            when disambiguating masks\n",
    "          activation (nn.Module): the type of activation to use when\n",
    "            upscaling masks\n",
    "          param_head_depth (int): the depth of the MLP used to predict\n",
    "            mask quality\n",
    "          param_head_hidden_dim (int): the hidden dimension of the MLP\n",
    "            used to predict mask quality\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "        self.num_param_tokens = num_params\n",
    "\n",
    "        # self.num_multimask_outputs = num_multimask_outputs\n",
    "        #\n",
    "        self.SSIM_token = nn.Embedding(1, transformer_dim)\n",
    "        # self.num_mask_tokens = num_multimask_outputs + 1\n",
    "        # self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n",
    "\n",
    "        self.output_upscaling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(transformer_dim, transformer_dim // (output_dim_factor // 2), kernel_size=2, stride=2),\n",
    "            LayerNorm2d(transformer_dim // (output_dim_factor // 2)),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(transformer_dim // (output_dim_factor // 2), transformer_dim // output_dim_factor, kernel_size=2, stride=2),\n",
    "            activation(),\n",
    "        )\n",
    "        # self.output_hypernetworks_mlps = nn.ModuleList(\n",
    "        #     [\n",
    "        #         MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n",
    "        #         for i in range(self.num_mask_tokens)\n",
    "        #     ]\n",
    "        # )\n",
    "        self.output_hypernetworks_mlp = MLP(transformer_dim * self.num_param_tokens, transformer_dim * self.num_param_tokens, transformer_dim // output_dim_factor, 3)\n",
    "        #\n",
    "        self.SSIM_prediction_head = MLP(\n",
    "            transformer_dim, SSIM_head_hidden_dim, 1, SSIM_head_depth\n",
    "        ) # return [b, num_params, 1]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict masks given image and prompt embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          image_embeddings (torch.Tensor): the embeddings from the image encoder\n",
    "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
    "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
    "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
    "          multimask_output (bool): Whether to return multiple masks or a single\n",
    "            mask.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: batched predicted masks\n",
    "          torch.Tensor: batched predictions of mask quality\n",
    "        \"\"\"\n",
    "        images = self.predict_images(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=image_pe,\n",
    "            sparse_prompt_embeddings=sparse_prompt_embeddings,\n",
    "        )\n",
    "\n",
    "        # Prepare output\n",
    "        return images #[B, 256/4, H, W]\n",
    "\n",
    "    def predict_images(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n",
    "        # Concatenate output tokens\n",
    "        output_tokens = self.SSIM_token.weight\n",
    "        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n",
    "        # tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
    "\n",
    "        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
    "        # Expand per-image data in batch direction to be per-mask\n",
    "        # src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
    "        # src = src + dense_prompt_embeddings\n",
    "        src = image_embeddings\n",
    "        # pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
    "        pos_src = image_pe\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        # Run the transformer\n",
    "        hs, src = self.transformer(src, pos_src, tokens)\n",
    "        # iou_token_out = hs[:, 0, :]\n",
    "        SSIM_token_out = hs[:, 0, :]\n",
    "        param_tokens_out = hs[:, 1 : , :]# [b, num_param, transformer_dim]\n",
    "        print_var_detail(param_tokens_out)\n",
    "        # param_tokens_out = hs # [b, num_param, transformer_dim]\n",
    "        param_tokens_out = param_tokens_out.view(b, 1, -1) # [b, 1, transformer_dim * num_param]\n",
    "\n",
    "\n",
    "\n",
    "        # Upscale mask embeddings and predict masks using the mask tokens\n",
    "        src = src.transpose(1, 2).view(b, c, h, w)\n",
    "        upscaled_embedding = self.output_upscaling(src)\n",
    "        # hyper_in_list: List[torch.Tensor] = []\n",
    "        # for i in range(self.num_mask_tokens):\n",
    "        #     hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
    "        # hyper_in = torch.stack(hyper_in_list, dim=1)\n",
    "        # b, c, h, w = upscaled_embedding.shape\n",
    "        # masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "        #\n",
    "        # # Generate mask quality predictions\n",
    "        SSIM_pred = self.SSIM_prediction_head(SSIM_token_out)\n",
    "        # b, c, h, w = upscaled_embedding.shape\n",
    "        # hyper_in = self.output_hypernetworks_mlp(param_tokens_out)\n",
    "        # image_decoded = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "\n",
    "        return upscaled_embedding, SSIM_pred\n",
    "\n",
    "    def _get_transformer_dim(self):\n",
    "        return self.transformer_dim\n",
    "\n",
    "\n",
    "# Lightly adapted from\n",
    "# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from modeling.transformer import TwoWayTransformer\n",
    "from help_func import print_var_detail\n",
    "image_decoder=ImageDecoder(\n",
    "            transformer=TwoWayTransformer(\n",
    "                depth=2,\n",
    "                embedding_dim=256,\n",
    "                mlp_dim=2048,\n",
    "                num_heads=8,\n",
    "            ),\n",
    "            transformer_dim=256,\n",
    "        )\n",
    "image_embeddings = torch.rand((5,256,64,64))\n",
    "image_pe = torch.rand((5,256,64,64))\n",
    "sparse_prompt_embeddings = torch.rand((5,1,256))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T01:12:02.717581500Z",
     "start_time": "2023-06-07T01:12:02.671074300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a  <class 'torch.Tensor'> with shape torch.Size([5, 1, 256]) max:  tensor(2.7620, grad_fn=<MaxBackward1>) min:  tensor(-3.2282, grad_fn=<MinBackward1>)\n",
      " is a  <class 'torch.Tensor'> with shape torch.Size([5, 32, 256, 256]) max:  tensor(1.1738, grad_fn=<MaxBackward1>) min:  tensor(-0.1700, grad_fn=<MinBackward1>)\n",
      " is a  <class 'torch.Tensor'> with shape torch.Size([5, 1]) max:  tensor(-0.0180, grad_fn=<MaxBackward1>) min:  tensor(-0.0348, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "y,ssim = image_decoder(image_embeddings = image_embeddings, image_pe = image_pe, sparse_prompt_embeddings = sparse_prompt_embeddings)\n",
    "print_var_detail(y)\n",
    "print_var_detail(ssim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-07T01:12:02.857247200Z",
     "start_time": "2023-06-07T01:12:02.718581500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
