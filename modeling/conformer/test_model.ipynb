{
 "cells": [
  {
   "cell_type": "code",
   "id": "c23de9a6b526cc91",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:06.235129Z",
     "start_time": "2024-09-10T06:11:06.226594Z"
    }
   },
   "source": [
    "def print_average_parameter(model):\n",
    "    total_params = 0\n",
    "    total_sum = 0.0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "        total_sum += param.sum().item()\n",
    "    average = total_sum / total_params\n",
    "    print(\"Average parameter value:\", average)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:10.987986Z",
     "start_time": "2024-09-10T06:11:06.237174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models import mae_vit_base_patch16_test, mae_vit_large_patch16,Transconv_small_patch16_test\n",
    "from transconv import TransConvRaw\n",
    "from mae.models_vit import VisionTransformer as MAE_VisionTransformer\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import copy\n",
    "from nets.mae.models_mae import mae_vit_base_patch16, mae_vit_large_patch16\n",
    "pretrain_vit_path = 'D:/Mengyu_Li/saved_models/mae_pretrain/mae_vit_base_patch16/model_latest.pt'\n",
    "pre_trained_vit = mae_vit_base_patch16()\n",
    "print_average_parameter(pre_trained_vit)\n",
    "pre_trained_vit.load_state_dict(torch.load(pretrain_vit_path)['model_state_dict'])\n",
    "print_average_parameter(pre_trained_vit)\n",
    "tasks = ['extraction']\n",
    "# image_vit = mae_vit_base_patch16_test(pretrained=pretrain_path)\n",
    "# image_encoder.load_state_dict(torch.load(pretrain_path)['model_state_dict'])\n",
    "# patch_size=16, channel_ratio=4, embed_dim=768 if pre_trained_vit is not None else 384, depth=12,\n",
    "#                         num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "#                         pre_trained_vit=pre_trained_vit, vit_depth=12, pre_trained_conformer=pre_trained_conformer,\n",
    "#                         additive_fusion_down=False, additive_fusion_up=False, up_ftr_map_size=[56]+[56]*3+[28]*4+[14]*4, down_ftr_map_size=[197]*12,\n",
    "                        \n",
    "# patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        \n",
    "model = TransConvRaw( patch_size=16, in_chans=3, base_channel=64, channel_ratio=4, num_med_block=0,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True,\n",
    "                 pre_trained_vit=pre_trained_vit, finetune_vit=True, vit_depth=12, pre_trained_conformer=None, finetune_conv=True,\n",
    "                 additive_fusion_down=False, additive_fusion_up=False, up_ftr_map_size=[56]+[56]*3+[28]*4+[14]*4, down_ftr_map_size=[197]*12,\n",
    "                 qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., decoder_embed_dim=512, out_chans=7)\n",
    "# model_tran = MAE_VisionTransformer(\n",
    "#         patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "#         norm_layer=partial(nn.LayerNorm, eps=1e-6), num_classes=1000, drop_path_rate=0.1,\n",
    "#         global_pool=True)\n",
    "# model_tran.load_state_dict(torch.load(pretrain_vit_path)['model_state_dict'], strict=False)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average parameter value: 0.0012862317514475254\n",
      "Average parameter value: 0.0008913620735884696\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:10.996716Z",
     "start_time": "2024-09-10T06:11:10.988991Z"
    }
   },
   "cell_type": "code",
   "source": "[56]+[56]*3+[28]*4+[14]*4",
   "id": "25e22c5d98df07ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 56, 56, 56, 28, 28, 28, 28, 14, 14, 14, 14]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:11.002456Z",
     "start_time": "2024-09-10T06:11:10.997721Z"
    }
   },
   "cell_type": "code",
   "source": "pre_trained_vit.blocks",
   "id": "2baf5dbab309cd53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-11): 12 x Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:11.023032Z",
     "start_time": "2024-09-10T06:11:11.003453Z"
    }
   },
   "cell_type": "code",
   "source": "print_average_parameter(pre_trained_vit)",
   "id": "e02007bc76aa7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average parameter value: 0.0008913620735884696\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:11.044764Z",
     "start_time": "2024-09-10T06:11:11.025036Z"
    }
   },
   "cell_type": "code",
   "source": "print_average_parameter(model.pre_trained_vit)",
   "id": "1d78f3b1369f1686",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average parameter value: 0.0008913620735884696\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:11.054324Z",
     "start_time": "2024-09-10T06:11:11.045753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "asd = torch.rand(8, 3, 224, 224)"
   ],
   "id": "41f8be619ea0ae0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.017466Z",
     "start_time": "2024-09-10T06:11:11.056334Z"
    }
   },
   "cell_type": "code",
   "source": "asd_out = model(asd)",
   "id": "1f62eff82b3ef9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t before decoder pred  torch.Size([8, 197, 512])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.022101Z",
     "start_time": "2024-09-10T06:11:13.017466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(9,13):\n",
    "    print(i)"
   ],
   "id": "6177ae1daef74aa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.030566Z",
     "start_time": "2024-09-10T06:11:13.022101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from help_func import print_var_detail\n",
    "asd_3 = torch.rand(8, 196, 224)\n",
    "\n",
    "x_masked, mask, ids_restore = pre_trained_vit.random_masking(asd_3, mask_ratio=0)\n",
    "pool = nn.AdaptiveAvgPool2d(1)\n",
    "asd_pool = pool(torch.rand(8, 1024, 7, 7))\n",
    "print(asd_pool.shape)"
   ],
   "id": "c0ac810ae45d772f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1024, 1, 1])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.034844Z",
     "start_time": "2024-09-10T06:11:13.030566Z"
    }
   },
   "cell_type": "code",
   "source": "print_var_detail(mask)",
   "id": "8e53af1a24dc6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is a  <class 'torch.Tensor'> with shape torch.Size([8, 196]) max:  tensor(0.) min:  tensor(0.)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.038376Z",
     "start_time": "2024-09-10T06:11:13.034844Z"
    }
   },
   "cell_type": "code",
   "source": "print(asd_out[0].shape)",
   "id": "4254071faf499e8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 7, 224, 224])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.041994Z",
     "start_time": "2024-09-10T06:11:13.038376Z"
    }
   },
   "cell_type": "code",
   "source": "print(asd_out[1].shape)",
   "id": "b56d86ef08a3061a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 7, 224, 224])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.045597Z",
     "start_time": "2024-09-10T06:11:13.041994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stack = asd_out[2]\n",
    "for x in stack[:-1]:\n",
    "    print_var_detail(x)"
   ],
   "id": "e712151367b3be5c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.048957Z",
     "start_time": "2024-09-10T06:11:13.046600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for i in range(4):\n",
    "#     downsample_layer = stack.pop()\n",
    "#     print_var_detail(downsample_layer)"
   ],
   "id": "b6895c67618c6a16",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.052552Z",
     "start_time": "2024-09-10T06:11:13.048957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# depth = 12\n",
    "# step = depth // 3\n",
    "# \n",
    "# stack_new = stack[:2]\n",
    "# \n",
    "# for i in range(2,len(stack)-1,step):\n",
    "#     print(i)\n",
    "#     end = min(i + step, len(stack)-1)\n",
    "#     stacked_tensor = torch.stack(stack[i:end])\n",
    "#     stack_new.append(torch.mean(stacked_tensor, dim=0))\n"
   ],
   "id": "259dcf9ab7fa4999",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.197005Z",
     "start_time": "2024-09-10T06:11:13.052552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ====== Construct model ======\n",
    "from modeling.unet import Unet\n",
    "_model = Unet(in_chans=3,out_chans=7, chans=64, if_classify= False,  dim=224, mlp_ratio = 32)\n"
   ],
   "id": "1bc945179c5cb612",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:13.999470Z",
     "start_time": "2024-09-10T06:11:13.197005Z"
    }
   },
   "cell_type": "code",
   "source": "asd_out_unet = _model(asd)",
   "id": "5e9747a2bdd0e775",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down:  torch.Size([8, 64, 224, 224])\n",
      "down:  torch.Size([8, 128, 112, 112])\n",
      "down:  torch.Size([8, 256, 56, 56])\n",
      "down:  torch.Size([8, 512, 28, 28])\n",
      "output:  torch.Size([8, 1024, 14, 14])\n",
      "up:  torch.Size([8, 512, 28, 28])\n",
      "output after cat:  torch.Size([8, 1024, 28, 28])\n",
      "up:  torch.Size([8, 256, 56, 56])\n",
      "output after cat:  torch.Size([8, 512, 56, 56])\n",
      "up:  torch.Size([8, 128, 112, 112])\n",
      "output after cat:  torch.Size([8, 256, 112, 112])\n",
      "up:  torch.Size([8, 64, 224, 224])\n",
      "output after cat:  torch.Size([8, 128, 224, 224])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T06:11:14.015111Z",
     "start_time": "2024-09-10T06:11:13.999470Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "dc48c04afe9a2ea3",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
