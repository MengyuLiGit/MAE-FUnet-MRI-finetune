{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.help_func import create_path, print_var_detail\n",
    "class SegmentationNpyDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(images_dir) if f.endswith('.npy')\n",
    "        ])\n",
    "        self.mask_files = sorted([\n",
    "            f for f in os.listdir(masks_dir) if f.endswith('.npy')\n",
    "        ])\n",
    "        assert len(self.image_files) == len(self.mask_files), \"Mismatch between image and mask counts\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(os.path.join(self.images_dir, self.image_files[idx]))  # [3, 224, 224]\n",
    "        mask = np.load(os.path.join(self.masks_dir, self.mask_files[idx]))     # [1, 224, 224]\n",
    "\n",
    "        image = torch.from_numpy(image).float()  # [3, H, W]\n",
    "        mask = torch.from_numpy(mask).long()     # [1, H, W]\n",
    "        return image, mask\n",
    "\n",
    "# === Initialize datasets ===\n",
    "train_dataset = SegmentationNpyDataset(\n",
    "    \"./finetune_data/skull_strip/train/images\",\n",
    "    \"./finetune_data/skull_strip/train/masks\"\n",
    ")\n",
    "test_dataset = SegmentationNpyDataset(\n",
    "    \"./finetune_data/skull_strip/test/images\",\n",
    "    \"./finetune_data/skull_strip/test/masks\"\n",
    ")\n",
    "\n",
    "# === Create DataLoaders ===\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "dataloader_val = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "images, masks = next(iter(dataloader_val))\n",
    "image_tensor = images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55f6bd5cde59e5cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "target_class = 1\n",
    "plot_index = 0\n",
    "print_var_detail(image_tensor[plot_index])\n",
    "plt.imshow(image_tensor[plot_index].to(dtype=torch.float32).permute(1, 2, 0), cmap='gray')\n",
    "plt.imshow((masks[plot_index] ==  target_class).permute(1, 2, 0), cmap='hot', alpha=0.4)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7d2ed34c5592244",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from multi_seg.models_mae_finetune_seg import MaskedAutoencoderViTMultiSeg\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "mode = 'mae_unet_fuse'\n",
    "INPUT_SIZE = 224\n",
    "mae_model = MaskedAutoencoderViTMultiSeg(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), num_classes=2, mode=mode,\n",
    "                 drop=0.1, attn_drop=0.1, drop_path=0.05)\n",
    "\n",
    "pretrain_path = \"./saved_models/mae_vit_base_patch16_pretrain_test0.75_E30/model_E30.pt\"\n",
    "\n",
    "missing, unexpected = mae_model.load_state_dict(torch.load(pretrain_path)['model_state_dict'], strict=False)  # strict=False ignores unmatched keys\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "block = mae_model.decoder_blocks  # pick any block you want\n",
    "\n",
    "num_params = sum(p.numel() for p in block.parameters())\n",
    "trainable_params = sum(p.numel() for p in block.parameters() if p.requires_grad)\n",
    "from multi_seg.mae_unet_fuse import UnetWithMAEFusion\n",
    "from modeling.unet import Unet\n",
    "CHANS = 64\n",
    "unet_model = Unet(in_chans=3,out_chans=2, chans=CHANS, if_classify= False,  dim=INPUT_SIZE, mlp_ratio = 32)\n",
    "model = UnetWithMAEFusion(unet=unet_model, mae_model=mae_model, custom_fusion_modes=['concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat'], mae_indices = [0, 2, 5, 8, 11, 8, 5, 2, 0], skip_connect = True, skip_type = 'concat')\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(next(model.parameters()).dtype)\n",
    "missing, unexpected = mae_model.load_state_dict(torch.load(pretrain_path)['model_state_dict'], strict=False)  # strict=False ignores unmatched keys"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "872a20e099b50fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from multi_seg.dice_loss_seg import CombinedGDLFocalCELoss, softmax_helper\n",
    "from multi_seg.dice_loss_seg import CombinedDiceFocalCELoss, softmax_helper\n",
    "# gdl Loss settings\n",
    "gdl_kwargs = {\n",
    "    'apply_nonlin': softmax_helper,   # apply softmax inside\n",
    "    'batch_dice': False,              # per sample Dice\n",
    "    'do_bg': True,                    # include background (class 0)\n",
    "    'smooth': 1e-5,                    # smoothing factor\n",
    "    'square': False,\n",
    "    'square_volumes': False,\n",
    "}\n",
    "\n",
    "# Dice Loss settings\n",
    "dice_kwargs = {\n",
    "    'apply_nonlin': softmax_helper,  # Apply softmax inside DiceLoss\n",
    "    'batch_dice': False,              # Per-sample dice (not batch-level)\n",
    "    'do_bg': True,                    # Include background class\n",
    "    'smooth': 1e-5                    # Smoothing factor\n",
    "}\n",
    "\n",
    "# Focal Loss settings\n",
    "focal_kwargs = {\n",
    "    'gamma': 2.0,\n",
    "    'alpha': 0.25,\n",
    "    'reduction': 'mean',\n",
    "    'apply_nonlin': softmax_helper    # Apply softmax inside FocalLoss too\n",
    "}\n",
    "\n",
    "# CrossEntropy Loss settings\n",
    "ce_kwargs = {\n",
    "    'reduction': 'mean'\n",
    "}\n",
    "\n",
    "# (Optional) Weights for each sub-loss (dice, focal, ce)\n",
    "weights = (1.0, 1.0, 1.0)\n",
    "\n",
    "# Initialize loss\n",
    "loss_fn = CombinedGDLFocalCELoss(\n",
    "    gdl_kwargs=gdl_kwargs,\n",
    "    focal_kwargs=focal_kwargs,\n",
    "    ce_kwargs=ce_kwargs,\n",
    "    weights=weights\n",
    ") # this kind of works, but will cause negative loss, probably caused by GDL\n",
    "\n",
    "# loss_fn = CombinedDiceFocalCELoss(\n",
    "#     dice_kwargs=dice_kwargs,\n",
    "#     focal_kwargs=focal_kwargs,\n",
    "#     ce_kwargs=ce_kwargs,\n",
    "#     weights=weights\n",
    "# )\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss(\n",
    "#     reduction='mean'\n",
    "# )\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Optimizer ----\n",
    "lr = 1e-4\n",
    "weight_decay= 1e-2\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=lr, \n",
    "    weight_decay=weight_decay\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "672557a446073429",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from multi_seg.train_mae_finetune_seg import Trainer\n",
    "freeze_mae_encoder = True\n",
    "freeze_mae_encoder_decoder = True\n",
    "force_float32 = False\n",
    "TRAIN_EPOCHS = 1000\n",
    "freeze_start_epoch = None\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if freeze_mae_encoder:\n",
    "    if freeze_mae_encoder_decoder:\n",
    "        path_save = \"../saved_models/skull_strip/syn_freeze_E_D\" +\"_mae_unet\"+ str(CHANS) + \"concat_\" + mode + \"_\" + \"_e\" + str(TRAIN_EPOCHS) +  '/'\n",
    "    else:\n",
    "        path_save = \"../saved_models/skull_strip/syn_freeze_E\" + \"_mae_unet\"+ str(CHANS) + \"concat_\" + mode + \"_\" + \"_e\" + str(TRAIN_EPOCHS) +  '/'\n",
    "else: # use \n",
    "    path_save = \"../saved_models/skull_strip/syn_unfreeze\" + \"_mae_unet\"+ str(CHANS) + \"concat_\" + mode +  \"_e\" + str(TRAIN_EPOCHS) +  '/'\n",
    "\n",
    "create_path(path_save)\n",
    "trainer = Trainer(\n",
    "    loader_train=dataloader_train,      # your training DataLoader\n",
    "    loader_test=dataloader_val,        # your test/validation DataLoader\n",
    "    my_model=model,                 # your MaskedAutoencoderViTMultiSeg model\n",
    "    my_loss=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    RESUME_EPOCH=0,                 # set >0 if you load checkpoints\n",
    "    PATH_MODEL=path_save,    # where you want to save checkpoints\n",
    "    device=device,                  # or 'cuda:0', depending on your setup\n",
    "    task='segment',                 # since you are doing segmentation\n",
    "    freeze_mae_encoder=freeze_mae_encoder,       # if you want to freeze encoder\n",
    "    freeze_mae_encoder_decoder=freeze_mae_encoder_decoder,  # if you want to freeze encoder+decoder\n",
    "    force_float32 = force_float32,\n",
    "    unfreeze_at_epoch = freeze_start_epoch,\n",
    "    weight_decay=weight_decay,\n",
    "    num_classes= 2,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e041c9a016d020",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = trainer.train(epochs=TRAIN_EPOCHS, show_step=200, show_test=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a280b124b2186df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_predictions_with_input(images, logits, gt_masks, selected_classes=None, max_plots=4):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images: (B, 3, 224, 224) input images (grayscale replicated to 3 channels)\n",
    "        logits: (B, 14, 224, 224) model output logits\n",
    "        gt_masks: (B, 1, 224, 224) ground truth masks\n",
    "        selected_classes: list of class indices to show (excluding background 0), e.g., [1,2,3]\n",
    "        max_plots: how many samples to plot\n",
    "    \"\"\"\n",
    "    preds = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(preds, dim=1)  # (B, 224, 224)\n",
    "\n",
    "    batch_size = images.size(0)\n",
    "    max_plots = min(max_plots, batch_size)\n",
    "\n",
    "    imgs = images.detach().cpu()\n",
    "    preds = preds.cpu()\n",
    "    gts = gt_masks.squeeze(1).cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(max_plots, 3, figsize=(15, 5 * max_plots))\n",
    "\n",
    "    if max_plots == 1:\n",
    "        axs = [axs]  # ensure iterable\n",
    "\n",
    "    for idx in range(max_plots):\n",
    "        img = imgs[idx, 0]  # assuming grayscale is in channel 0\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # normalize for display\n",
    "\n",
    "        pred_mask = preds[idx].numpy()\n",
    "        gt_mask = gts[idx].numpy()\n",
    "\n",
    "        # Mask selection\n",
    "        if selected_classes is not None:\n",
    "            pred_mask = np.where(np.isin(pred_mask, selected_classes), pred_mask, 0)\n",
    "            gt_mask = np.where(np.isin(gt_mask, selected_classes), gt_mask, 0)\n",
    "\n",
    "        pred_mask_masked = np.ma.masked_where(pred_mask == 0, pred_mask)\n",
    "        gt_mask_masked = np.ma.masked_where(gt_mask == 0, gt_mask)\n",
    "\n",
    "        # --- Input MRI\n",
    "        axs[idx][0].imshow(img, cmap='gray')\n",
    "        axs[idx][0].set_title(f\"Input MRI (Image {idx})\")\n",
    "        axs[idx][0].axis('off')\n",
    "\n",
    "        # --- Prediction\n",
    "        axs[idx][1].imshow(img, cmap='gray')\n",
    "        axs[idx][1].imshow(pred_mask_masked, cmap='jet', alpha=0.5, vmin=1, vmax=13)\n",
    "        axs[idx][1].set_title(f\"Prediction Overlay (Image {idx})\")\n",
    "        axs[idx][1].axis('off')\n",
    "\n",
    "        # --- Ground truth\n",
    "        axs[idx][2].imshow(img, cmap='gray')\n",
    "        axs[idx][2].imshow(gt_mask_masked, cmap='jet', alpha=0.5, vmin=1, vmax=13)\n",
    "        axs[idx][2].set_title(f\"Ground Truth Overlay (Image {idx})\")\n",
    "        axs[idx][2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return pred_mask_masked, gt_mask_masked\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b132f0f198b2fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "images = images.to(device)\n",
    "logits = model(images.float())\n",
    "pred_mask_masked, gt_mask_masked = plot_predictions_with_input(image_tensor, logits, masks,selected_classes = [1],max_plots=8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "227de3a93c1669b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8ee3b71737fc9af",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
