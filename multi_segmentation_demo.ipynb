{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_and_plot_png(image_png_path, mask_png_path):\n",
    "    # === Load PNGs\n",
    "    img = imageio.imread(image_png_path)  # shape: [H, W], dtype: uint8\n",
    "    mask = imageio.imread(mask_png_path)  # shape: [H, W], dtype: uint8 (raw class indices)\n",
    "\n",
    "    print(f\"Image: shape={img.shape}, dtype={img.dtype}, min={img.min()}, max={img.max()}\")\n",
    "    print(f\"Mask : shape={mask.shape}, dtype={mask.dtype}, unique labels={np.unique(mask)}\")\n",
    "\n",
    "    # === Plot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axs[0].imshow(img, cmap='gray')\n",
    "    axs[0].set_title(\"Grayscale Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(mask, cmap='tab20')  # simple colormap to see labels\n",
    "    axs[1].set_title(\"Segmentation Mask (raw labels)\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# labels from 1-13 corresponding to\n",
    "labels = ['Left/Right-Cerebral-White-Matter', 'Left/Right-Cerebral-Cortex',\n",
    "'Left/Right-Cerebellum-White-Matter', 'Left/Right-Cerebellum-Cortex', 'Left/Right-Thalamus', 'Left/Right-Caudate',\n",
    "'Left/Right-Putamen', 'Left/Right-Pallidum', 'Brain-Stem ', 'Left/Right-Hippocampus', 'Left/Right-Amygdala', 'CSF',\n",
    "'WM-hypointensities']\n",
    "load_and_plot_png(\"./demo_data/multi_segementation/img1.png\", \"./demo_data/multi_segementation/mask1.png\")\n"
   ],
   "id": "7a18e99a268431b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from multi_seg.models_mae_finetune_seg import MaskedAutoencoderViTMultiSeg\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "mode = 'mae_unet_fuse'\n",
    "INPUT_SIZE = 224\n",
    "# mode = 'CNN'\n",
    "mae_model = MaskedAutoencoderViTMultiSeg(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), num_classes=len(labels) + 1, mode=mode,\n",
    "                 drop=0.1, attn_drop=0.1, drop_path=0.05)\n",
    "\n",
    "pretrain_path = \"./saved_models/mae_vit_base_patch16_pretrain_test0.75_E30/model_E30.pt\"\n",
    "\n",
    "missing, unexpected = mae_model.load_state_dict(torch.load(pretrain_path)['model_state_dict'], strict=False)  # strict=False ignores unmatched keys\n",
    "\n",
    "from multi_seg.mae_unet_fuse import UnetWithMAEFusion\n",
    "from modeling.unet import Unet\n",
    "CHANS = 64\n",
    "unet_model = Unet(in_chans=3,out_chans=len(labels) + 1, chans=CHANS, if_classify= False,  dim=INPUT_SIZE, mlp_ratio = 32)\n",
    "\n",
    "MAEFusion = UnetWithMAEFusion(unet=unet_model, mae_model=mae_model, custom_fusion_modes=['concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat'], mae_indices = [0, 2, 5, 8, 11, 8, 5, 2, 0], skip_connect = True, skip_type='concat')\n",
    "print(sum(p.numel() for p in MAEFusion.parameters()))\n",
    "print(sum(p.numel() for p in MAEFusion.parameters() if p.requires_grad))\n",
    "print(next(MAEFusion.parameters()).dtype)\n",
    "MAEFusion.load_state_dict(torch.load(\"./finetune_models/multi_segmentation/mae_unet64concat_mae_unet_fuse_e1000_30.pt\")['model_state_dict'], strict=False)"
   ],
   "id": "1b1d97769b5605cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define class color list (indexed from class 0 to 13)\n",
    "# You can change these RGB values (range 0–1 or 0–255 normalized)\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "class_colors = [\n",
    "    (0.0, 0.0, 0.0),       # 0: Black (Background)\n",
    "    (0.6, 0.3, 0.0),       # 1: Brown\n",
    "    (0.85, 0.1, 0.1),      # 2: Red\n",
    "    (0.7, 0.7, 0.7),       # 3: Light Gray\n",
    "    (0.0, 0.3, 0.3),       # 4: Teal-Green\n",
    "    (0.5, 0.0, 0.5),       # 5: Purple\n",
    "    (0.9, 0.6, 0.0),       # 6: Orange\n",
    "    (0.3, 0.3, 0.3),       # 7: Dark Gray\n",
    "    (0.0, 0.6, 0.0),       # 8: Dark Green\n",
    "    (0.6, 0.0, 0.6),       # 9: Magenta\n",
    "    (0.1, 0.1, 0.85),      # 10: Blue\n",
    "    (0.9, 0.2, 0.5),       # 11: Deep Pink\n",
    "    (0.2, 0.5, 0.7),       # 12: Steel Blue\n",
    "    (0.9, 0.9, 0.0),       # 13: Yellow\n",
    "]\n",
    "\n",
    "# Normalize RGB values if they are in 0–255\n",
    "class_colors = [(r, g, b) if max((r, g, b)) <= 1 else (r / 255, g / 255, b / 255)\n",
    "                for (r, g, b) in class_colors]\n",
    "\n",
    "custom_cmap = ListedColormap(class_colors)\n",
    "custom_norm = BoundaryNorm(boundaries=np.arange(15) - 0.5, ncolors=14)\n",
    "def plot_single_png_prediction(\n",
    "    image_png_path, mask_png_path,\n",
    "    models, model_names,\n",
    "    selected_classes=None,\n",
    "    device='cuda',\n",
    "    save_path=None\n",
    "):\n",
    "    assert len(models) == len(model_names), \"Each model must have a name\"\n",
    "\n",
    "    # Load image (grayscale, [H, W])\n",
    "    img = imageio.imread(image_png_path).astype(np.float32) / 255.0\n",
    "    mask = imageio.imread(mask_png_path).astype(np.int64)  # [H, W]\n",
    "\n",
    "    # Prepare input tensor\n",
    "    img_tensor = torch.tensor(img).unsqueeze(0).repeat(3, 1, 1).unsqueeze(0).to(device)  # [1, 3, H, W]\n",
    "\n",
    "    # Load models to device\n",
    "    for i in range(len(models)):\n",
    "        models[i] = models[i].to(device)\n",
    "        models[i].eval()\n",
    "\n",
    "    # Predict\n",
    "    preds_all = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            logits = model(img_tensor)  # [1, C, H, W]\n",
    "            pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "            preds_all.append(pred)\n",
    "\n",
    "    # Prepare GT\n",
    "    gt_mask = mask\n",
    "    if selected_classes is not None:\n",
    "        gt_mask = np.where(np.isin(gt_mask, selected_classes), gt_mask, 0)\n",
    "    gt_mask_masked = np.ma.masked_where(gt_mask == 0, gt_mask)\n",
    "\n",
    "    # === Plotting: raw + GT + N predictions\n",
    "    num_cols = 2 + len(models)\n",
    "    fig, axs = plt.subplots(1, num_cols, figsize=(4 * num_cols, 5))\n",
    "\n",
    "    # Raw MRI\n",
    "    axs[0].imshow(img, cmap='gray')\n",
    "    axs[0].set_title(\"Raw MRI\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # GT overlay\n",
    "    axs[1].imshow(img, cmap='gray')\n",
    "    axs[1].imshow(gt_mask_masked, cmap=custom_cmap, norm=custom_norm, alpha=0.5)\n",
    "    axs[1].set_title(\"Ground Truth\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Model predictions\n",
    "    for m_idx, (pred_mask, name) in enumerate(zip(preds_all, model_names)):\n",
    "        if selected_classes is not None:\n",
    "            pred_mask = np.where(np.isin(pred_mask, selected_classes), pred_mask, 0)\n",
    "\n",
    "        pred_mask_masked = np.ma.masked_where(pred_mask == 0, pred_mask)\n",
    "\n",
    "        axs[2 + m_idx].imshow(img, cmap='gray')\n",
    "        axs[2 + m_idx].imshow(pred_mask_masked, cmap=custom_cmap, norm=custom_norm, alpha=0.5)\n",
    "        axs[2 + m_idx].set_title(f\"{name} Prediction\")\n",
    "        axs[2 + m_idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n"
   ],
   "id": "23259ea94e59bd6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output_path = None\n",
    "plot_single_png_prediction(\n",
    "    image_png_path=\"./demo_data/multi_segementation/img1.png\",\n",
    "    mask_png_path=\"./demo_data/multi_segementation/mask1.png\",\n",
    "    models=[MAEFusion],\n",
    "    model_names=[\"MAE-FUnet\"],\n",
    "    device='cuda'\n",
    ")\n"
   ],
   "id": "36e81ac42433d5a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "113ea532382a8d76",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
