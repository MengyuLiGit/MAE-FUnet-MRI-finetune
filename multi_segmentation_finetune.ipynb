{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.help_func import create_path\n",
    "class SegmentationNpyDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(images_dir) if f.endswith('.npy')\n",
    "        ])\n",
    "        self.mask_files = sorted([\n",
    "            f for f in os.listdir(masks_dir) if f.endswith('.npy')\n",
    "        ])\n",
    "        assert len(self.image_files) == len(self.mask_files), \"Mismatch between image and mask counts\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(os.path.join(self.images_dir, self.image_files[idx]))  # [3, 224, 224]\n",
    "        mask = np.load(os.path.join(self.masks_dir, self.mask_files[idx]))     # [1, 224, 224]\n",
    "\n",
    "        image = torch.from_numpy(image).float()  # [3, H, W]\n",
    "        mask = torch.from_numpy(mask).long()     # [1, H, W]\n",
    "        return image, mask\n",
    "\n",
    "# === Initialize datasets ===\n",
    "train_dataset = SegmentationNpyDataset(\n",
    "    \"./finetune_data/segmentation/train/images\",\n",
    "    \"./finetune_data/segmentation/train/masks\"\n",
    ")\n",
    "test_dataset = SegmentationNpyDataset(\n",
    "    \"./finetune_data/segmentation/test/images\",\n",
    "    \"./finetune_data/segmentation/test/masks\"\n",
    ")\n",
    "\n",
    "# === Create DataLoaders ===\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from multi_seg.models_mae_finetune_seg import MaskedAutoencoderViTMultiSeg\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "labels = ['Left/Right-Cerebral-White-Matter', 'Left/Right-Cerebral-Cortex',\n",
    "'Left/Right-Cerebellum-White-Matter', 'Left/Right-Cerebellum-Cortex', 'Left/Right-Thalamus', 'Left/Right-Caudate',\n",
    "'Left/Right-Putamen', 'Left/Right-Pallidum', 'Brain-Stem ', 'Left/Right-Hippocampus', 'Left/Right-Amygdala', 'CSF',\n",
    "'WM-hypointensities']\n",
    "mode = 'mae_unet_fuse'\n",
    "INPUT_SIZE = 224\n",
    "# mode = 'CNN'\n",
    "mae_model = MaskedAutoencoderViTMultiSeg(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), num_classes=len(labels) + 1, mode=mode,\n",
    "                 drop=0.1, attn_drop=0.1, drop_path=0.05)\n",
    "\n",
    "pretrain_path = './saved_models/mae_vit_base_patch16_pretrain_test0.75_E30/model_E30.pt'\n",
    "\n",
    "missing, unexpected = mae_model.load_state_dict(torch.load(pretrain_path)['model_state_dict'], strict=False)  # strict=False ignores unmatched keys\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "from multi_seg.mae_unet_fuse import UnetWithMAEFusion\n",
    "from modeling.unet import Unet\n",
    "CHANS = 64\n",
    "unet_model = Unet(in_chans=3,out_chans=len(labels) + 1, chans=CHANS, if_classify= False,  dim=INPUT_SIZE, mlp_ratio = 32)\n",
    "\n",
    "model = UnetWithMAEFusion(unet=unet_model, mae_model=mae_model, custom_fusion_modes=['concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat', 'concat'], mae_indices = [0, 2, 5, 8, 11, 8, 5, 2, 0], skip_connect = True, skip_type = 'concat')"
   ],
   "id": "23a7001c842b78a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from multi_seg.dice_loss_seg import CombinedGDLFocalCELoss, softmax_helper\n",
    "from multi_seg.dice_loss_seg import CombinedDiceFocalCELoss, softmax_helper\n",
    "# gdl Loss settings\n",
    "gdl_kwargs = {\n",
    "    'apply_nonlin': softmax_helper,   # apply softmax inside\n",
    "    'batch_dice': False,              # per sample Dice\n",
    "    'do_bg': True,                    # include background (class 0)\n",
    "    'smooth': 1e-5,                    # smoothing factor\n",
    "    'square': False,\n",
    "    'square_volumes': False,\n",
    "}\n",
    "\n",
    "# Dice Loss settings\n",
    "dice_kwargs = {\n",
    "    'apply_nonlin': softmax_helper,  # Apply softmax inside DiceLoss\n",
    "    'batch_dice': False,              # Per-sample dice (not batch-level)\n",
    "    'do_bg': True,                    # Include background class\n",
    "    'smooth': 1e-5                    # Smoothing factor\n",
    "}\n",
    "\n",
    "# Focal Loss settings\n",
    "focal_kwargs = {\n",
    "    'gamma': 2.0,\n",
    "    'alpha': 0.25,\n",
    "    'reduction': 'mean',\n",
    "    'apply_nonlin': softmax_helper    # Apply softmax inside FocalLoss too\n",
    "}\n",
    "\n",
    "# CrossEntropy Loss settings\n",
    "ce_kwargs = {\n",
    "    'reduction': 'mean'\n",
    "}\n",
    "\n",
    "# (Optional) Weights for each sub-loss (dice, focal, ce)\n",
    "weights = (1.0, 1.0, 1.0)\n",
    "\n",
    "\n",
    "loss_fn = CombinedDiceFocalCELoss(\n",
    "    dice_kwargs=dice_kwargs,\n",
    "    focal_kwargs=focal_kwargs,\n",
    "    ce_kwargs=ce_kwargs,\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss(\n",
    "#     reduction='mean'\n",
    "# )\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---- Optimizer ----\n",
    "lr = 1e-4\n",
    "weight_decay= 1e-2\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n"
   ],
   "id": "c1dd12d7e408daef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from multi_seg.train_mae_finetune_seg import Trainer\n",
    "freeze_mae_encoder = True\n",
    "freeze_mae_encoder_decoder = True\n",
    "force_float32 = False\n",
    "TRAIN_EPOCHS = 200\n",
    "freeze_start_epoch = None\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if freeze_mae_encoder:\n",
    "    if freeze_mae_encoder_decoder:\n",
    "        path_save = \"../saved_models/segment/freeze_ED\" + str(freeze_start_epoch) +\"_mae_unet\" + str(CHANS) + \"concat_\" + mode + \"_e\" + str(TRAIN_EPOCHS) + '/'\n",
    "    else:#concat_fuze_\n",
    "        path_save = \"../saved_models/segment/freeze_E\" + str(freeze_start_epoch) +\"_mae_unet\" + str(CHANS) + \"concat_\" + mode + \"_e\" + str(TRAIN_EPOCHS) + '/'\n",
    "else: # use\n",
    "    path_save = \"../saved_models/segment/unfreeze\" + str(freeze_start_epoch) +\"_mae_unet\" + str(CHANS) + \"concat_\" + mode + \"_e\" + str(TRAIN_EPOCHS) + '/'\n",
    "create_path(path_save)\n",
    "trainer = Trainer(\n",
    "    loader_train=dataloader_train,      # your training DataLoader\n",
    "    loader_test=dataloader_test,        # your test/validation DataLoader\n",
    "    my_model=model,                 # your MaskedAutoencoderViTMultiSeg model\n",
    "    my_loss=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    RESUME_EPOCH=0,                 # set >0 if you load checkpoints\n",
    "    PATH_MODEL=path_save,    # where you want to save checkpoints\n",
    "    device=device,                  # or 'cuda:0', depending on your setup\n",
    "    task='segment',                 # since you are doing segmentation\n",
    "    freeze_mae_encoder=freeze_mae_encoder,       # if you want to freeze encoder\n",
    "    freeze_mae_encoder_decoder=freeze_mae_encoder_decoder,  # if you want to freeze encoder+decoder\n",
    "    force_float32 = force_float32,\n",
    "    unfreeze_at_epoch = freeze_start_epoch,\n",
    "    weight_decay=weight_decay,\n",
    "    num_classes= len(labels) + 1,\n",
    ")\n"
   ],
   "id": "cb089513b26f1695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "model = trainer.train(epochs=TRAIN_EPOCHS, show_step=200, show_test=True)",
   "id": "963730632c51d290",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "38653b229ee23dd0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
